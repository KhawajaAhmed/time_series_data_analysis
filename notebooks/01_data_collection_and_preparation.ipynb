{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transportation Time Series Analysis - Data Collection and Preparation\n",
    "\n",
    "This notebook covers the first phase of our transportation time series analysis project:\n",
    "1. Project initiation and problem definition\n",
    "2. Data collection and loading\n",
    "3. Initial data exploration\n",
    "4. Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Initiation\n",
    "\n",
    "### Problem Statement\n",
    "We are analyzing transportation data to optimize fleet utilization and reduce idle time for a transportation company. The company operates a fleet of vehicles and wants to understand patterns in usage to better allocate resources.\n",
    "\n",
    "### Business Objectives\n",
    "- Identify patterns in vehicle usage across different time periods (daily, weekly, monthly)\n",
    "- Detect anomalies in vehicle utilization that may indicate inefficiencies\n",
    "- Forecast future demand to optimize fleet size and deployment\n",
    "- Reduce idle time by 15% within 6 months\n",
    "\n",
    "### Success Criteria\n",
    "- Accurate forecasting model with MAPE < 10%\n",
    "- Identification of at least 3 actionable insights for fleet optimization\n",
    "- Clear visualization of usage patterns for stakeholder presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom utility functions\n",
    "from src.data_utils import (\n",
    "    load_data,\n",
    "    check_missing_values,\n",
    "    handle_missing_values,\n",
    "    detect_outliers,\n",
    "    convert_to_datetime,\n",
    "    plot_time_series\n",
    ")\n",
    "\n",
    "from src.visualization_utils import (\n",
    "    set_plotting_style,\n",
    "    plot_distribution,\n",
    "    plot_correlation_heatmap,\n",
    "    plot_boxplot\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "set_plotting_style()\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection\n",
    "\n",
    "For this project, we'll generate synthetic transportation data that mimics real-world patterns. In a real project, you would load data from files, databases, or APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_transportation_data(start_date='2022-01-01', end_date='2023-12-31'):\n",
    "    \"\"\"\n",
    "    Generate synthetic transportation data with realistic patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    start_date : str\n",
    "        Start date for the dataset\n",
    "    end_date : str\n",
    "        End date for the dataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Synthetic transportation dataset\n",
    "    \"\"\"\n",
    "    # Create date range\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    n_days = len(date_range)\n",
    "    \n",
    "    # Base demand with weekly seasonality\n",
    "    weekday_effect = np.array([1.2, 1.3, 1.4, 1.3, 1.5, 0.8, 0.7])  # Mon-Sun\n",
    "    weekday_indices = np.array([d.weekday() for d in date_range])\n",
    "    base_demand = np.array([weekday_effect[i] for i in weekday_indices])\n",
    "    \n",
    "    # Add monthly seasonality\n",
    "    month_indices = np.array([d.month for d in date_range])\n",
    "    monthly_effect = np.sin(np.pi * month_indices / 6) * 0.3 + 1.0  # Peak in summer\n",
    "    \n",
    "    # Add yearly trend (increasing)\n",
    "    yearly_trend = np.linspace(0, 0.5, n_days)\n",
    "    \n",
    "    # Add holidays effect (major US holidays)\n",
    "    holidays = [\n",
    "        '2022-01-01', '2022-07-04', '2022-11-24', '2022-12-25',  # 2022 holidays\n",
    "        '2023-01-01', '2023-07-04', '2023-11-23', '2023-12-25'   # 2023 holidays\n",
    "    ]\n",
    "    holiday_effect = np.zeros(n_days)\n",
    "    for holiday in holidays:\n",
    "        holiday_idx = np.where(date_range == holiday)[0]\n",
    "        if len(holiday_idx) > 0:\n",
    "            holiday_effect[holiday_idx[0]] = -0.5  # Reduced demand on holidays\n",
    "    \n",
    "    # Combine all effects\n",
    "    demand = (base_demand * monthly_effect + yearly_trend + holiday_effect) * 100\n",
    "    \n",
    "    # Add random noise\n",
    "    noise = np.random.normal(0, 10, n_days)\n",
    "    demand = demand + noise\n",
    "    \n",
    "    # Add some outliers\n",
    "    outlier_indices = np.random.choice(n_days, size=int(n_days * 0.02), replace=False)\n",
    "    outlier_effect = np.random.choice([-50, 50], size=len(outlier_indices))\n",
    "    demand[outlier_indices] += outlier_effect\n",
    "    \n",
    "    # Ensure no negative values\n",
    "    demand = np.maximum(demand, 10)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'date': date_range,\n",
    "        'vehicle_usage_hours': demand.round(2),\n",
    "        'idle_time_hours': (24 - demand/10).round(2),\n",
    "        'fuel_consumption_gallons': (demand * 0.5 + np.random.normal(0, 5, n_days)).round(2),\n",
    "        'maintenance_cost': (demand * 2 + np.random.normal(0, 20, n_days)).round(2),\n",
    "        'distance_miles': (demand * 15 + np.random.normal(0, 50, n_days)).round(2)\n",
    "    })\n",
    "    \n",
    "    # Add vehicle types\n",
    "    vehicle_types = ['Sedan', 'SUV', 'Van', 'Truck']\n",
    "    df['vehicle_type'] = np.random.choice(vehicle_types, size=n_days)\n",
    "    \n",
    "    # Add regions\n",
    "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "    df['region'] = np.random.choice(regions, size=n_days)\n",
    "    \n",
    "    # Add driver_id\n",
    "    n_drivers = 20\n",
    "    df['driver_id'] = np.random.randint(1, n_drivers + 1, size=n_days)\n",
    "    \n",
    "    # Add some missing values\n",
    "    missing_indices = np.random.choice(n_days, size=int(n_days * 0.05), replace=False)\n",
    "    df.loc[missing_indices, 'fuel_consumption_gallons'] = np.nan\n",
    "    \n",
    "    missing_indices = np.random.choice(n_days, size=int(n_days * 0.03), replace=False)\n",
    "    df.loc[missing_indices, 'maintenance_cost'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "transportation_data = generate_synthetic_transportation_data()\n",
    "\n",
    "# Save the data to CSV\n",
    "data_dir = os.path.join('..', 'data', 'raw')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "transportation_data.to_csv(os.path.join(data_dir, 'transportation_data.csv'), index=False)\n",
    "\n",
    "print(f\"Generated synthetic transportation data with {len(transportation_data)} records.\")\n",
    "transportation_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration\n",
    "\n",
    "Let's explore the dataset to understand its structure, patterns, and potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = load_data(os.path.join(data_dir, 'transportation_data.csv'))\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_info = check_missing_values(df)\n",
    "print(\"Missing Values:\")\n",
    "missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime\n",
    "df = convert_to_datetime(df, 'date', set_index=True)\n",
    "\n",
    "# Plot time series for vehicle usage hours\n",
    "plot_time_series(df, 'vehicle_usage_hours', title='Vehicle Usage Hours Over Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of key metrics\n",
    "numeric_cols = ['vehicle_usage_hours', 'idle_time_hours', 'fuel_consumption_gallons', \n",
    "                'maintenance_cost', 'distance_miles']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plot_distribution(df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap\n",
    "plot_correlation_heatmap(df, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze patterns by vehicle type\n",
    "vehicle_type_stats = df.groupby('vehicle_type')[numeric_cols].mean()\n",
    "vehicle_type_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots by vehicle type\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.boxplot(x='vehicle_type', y=col, data=df.reset_index())\n",
    "    plt.title(f'{col} by Vehicle Type')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weekly patterns\n",
    "df['dayofweek'] = df.index.dayofweek\n",
    "df['dayname'] = df.index.day_name()\n",
    "\n",
    "weekly_patterns = df.groupby('dayname')[numeric_cols].mean()\n",
    "# Reorder days\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekly_patterns = weekly_patterns.reindex(day_order)\n",
    "weekly_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot weekly patterns\n",
    "plt.figure(figsize=(12, 6))\n",
    "weekly_patterns['vehicle_usage_hours'].plot(kind='bar')\n",
    "plt.title('Average Vehicle Usage Hours by Day of Week')\n",
    "plt.ylabel('Hours')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze monthly patterns\n",
    "df['month'] = df.index.month\n",
    "df['month_name'] = df.index.month_name()\n",
    "\n",
    "monthly_patterns = df.groupby('month_name')[numeric_cols].mean()\n",
    "# Reorder months\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "monthly_patterns = monthly_patterns.reindex(month_order)\n",
    "monthly_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot monthly patterns\n",
    "plt.figure(figsize=(14, 6))\n",
    "monthly_patterns['vehicle_usage_hours'].plot(kind='bar')\n",
    "plt.title('Average Vehicle Usage Hours by Month')\n",
    "plt.ylabel('Hours')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df_clean = handle_missing_values(df, method='interpolate')\n",
    "\n",
    "# Check if missing values were handled\n",
    "missing_after = check_missing_values(df_clean)\n",
    "print(\"Missing Values After Handling:\")\n",
    "missing_after if not missing_after.empty else print(\"No missing values remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers\n",
    "outliers = detect_outliers(df_clean, numeric_cols, method='zscore', threshold=3.0)\n",
    "\n",
    "# Count outliers in each column\n",
    "outlier_counts = {col: sum(outliers[col]) for col in outliers}\n",
    "print(\"Outlier Counts:\")\n",
    "for col, count in outlier_counts.items():\n",
    "    print(f\"{col}: {count} outliers ({count/len(df_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for vehicle_usage_hours\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(df_clean.index, df_clean['vehicle_usage_hours'], \n",
    "            c=['red' if x else 'blue' for x in outliers['vehicle_usage_hours']], alpha=0.5)\n",
    "plt.title('Vehicle Usage Hours with Outliers Highlighted')\n",
    "plt.ylabel('Hours')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers by capping\n",
    "def cap_outliers(df, columns, method='zscore', threshold=3.0):\n",
    "    \"\"\"\n",
    "    Cap outliers at upper and lower bounds.\n",
    "    \"\"\"\n",
    "    df_capped = df.copy()\n",
    "    outliers = detect_outliers(df, columns, method=method, threshold=threshold)\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in outliers:\n",
    "            if method == 'zscore':\n",
    "                mean = df[col].mean()\n",
    "                std = df[col].std()\n",
    "                lower_bound = mean - threshold * std\n",
    "                upper_bound = mean + threshold * std\n",
    "            elif method == 'iqr':\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - threshold * IQR\n",
    "                upper_bound = Q3 + threshold * IQR\n",
    "            \n",
    "            # Cap outliers\n",
    "            df_capped.loc[df_capped[col] < lower_bound, col] = lower_bound\n",
    "            df_capped.loc[df_capped[col] > upper_bound, col] = upper_bound\n",
    "    \n",
    "    return df_capped\n",
    "\n",
    "# Apply outlier capping\n",
    "df_clean = cap_outliers(df_clean, numeric_cols)\n",
    "\n",
    "# Check if outliers were handled\n",
    "outliers_after = detect_outliers(df_clean, numeric_cols, method='zscore', threshold=3.0)\n",
    "outlier_counts_after = {col: sum(outliers_after[col]) for col in outliers_after}\n",
    "print(\"Outlier Counts After Capping:\")\n",
    "for col, count in outlier_counts_after.items():\n",
    "    print(f\"{col}: {count} outliers ({count/len(df_clean)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional features\n",
    "df_clean['is_weekend'] = df_clean['dayofweek'].isin([5, 6]).astype(int)\n",
    "df_clean['is_holiday'] = 0  # We would populate this with actual holiday data\n",
    "df_clean['season'] = pd.cut(df_clean['month'], bins=[0, 3, 6, 9, 12], \n",
    "                          labels=['Winter', 'Spring', 'Summer', 'Fall'], \n",
    "                          include_lowest=True)\n",
    "\n",
    "# Calculate efficiency metrics\n",
    "df_clean['miles_per_gallon'] = df_clean['distance_miles'] / df_clean['fuel_consumption_gallons']\n",
    "df_clean['cost_per_mile'] = df_clean['maintenance_cost'] / df_clean['distance_miles']\n",
    "df_clean['utilization_rate'] = df_clean['vehicle_usage_hours'] / 24\n",
    "\n",
    "# Display the processed data\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data\n",
    "processed_dir = os.path.join('..', 'data', 'processed')\n",
    "if not os.path.exists(processed_dir):\n",
    "    os.makedirs(processed_dir)\n",
    "    \n",
    "df_clean.to_csv(os.path.join(processed_dir, 'transportation_data_processed.csv'))\n",
    "print(f\"Saved processed data to {os.path.join(processed_dir, 'transportation_data_processed.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initial Findings\n",
    "\n",
    "Based on our initial exploration, here are some key findings:\n",
    "\n",
    "1. **Temporal Patterns**:\n",
    "   - Weekday usage is significantly higher than weekend usage\n",
    "   - There's a seasonal pattern with higher usage during summer months\n",
    "   - There's an increasing trend in vehicle usage over the analyzed period\n",
    "\n",
    "2. **Vehicle Type Insights**:\n",
    "   - Different vehicle types show distinct usage patterns\n",
    "   - [Vehicle type with highest usage] has the highest utilization rate\n",
    "   - [Vehicle type with lowest usage] has the lowest utilization rate\n",
    "\n",
    "3. **Efficiency Metrics**:\n",
    "   - Strong correlation between vehicle usage hours and fuel consumption\n",
    "   - Maintenance costs increase with vehicle usage\n",
    "   - Idle time is inversely related to vehicle usage\n",
    "\n",
    "4. **Data Quality**:\n",
    "   - Successfully handled missing values through interpolation\n",
    "   - Identified and capped outliers to improve data quality\n",
    "   - Created additional features to enhance analysis capabilities\n",
    "\n",
    "In the next notebook, we'll perform more detailed exploratory data analysis, including time series decomposition and statistical tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
