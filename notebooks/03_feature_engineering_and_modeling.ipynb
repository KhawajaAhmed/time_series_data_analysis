{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transportation Time Series Analysis - Feature Engineering and Modeling\n",
    "\n",
    "This notebook covers the third phase of our transportation time series analysis project:\n",
    "1. Feature engineering for time series data\n",
    "2. Model development and training\n",
    "3. Model evaluation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Modeling libraries\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import pmdarima as pm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Add the project root directory to the Python path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom utility functions\n",
    "from src.data_utils import load_data, convert_to_datetime\n",
    "from src.visualization_utils import set_plotting_style, plot_multiple_time_series\n",
    "from src.modeling_utils import (\n",
    "    create_features,\n",
    "    train_test_split_time,\n",
    "    evaluate_forecast,\n",
    "    plot_forecast,\n",
    "    fit_arima,\n",
    "    auto_arima\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "set_plotting_style()\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data from Phase 1\n",
    "processed_dir = os.path.join('..', 'data', 'processed')\n",
    "df = pd.read_csv(os.path.join(processed_dir, 'transportation_data_processed.csv'))\n",
    "\n",
    "# Convert the index to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.set_index('date')\n",
    "\n",
    "# Load analysis results from Phase 2\n",
    "with open(os.path.join(processed_dir, 'analysis_results.json'), 'r') as f:\n",
    "    analysis_results = json.load(f)\n",
    "\n",
    "print(f\"Loaded processed data with {len(df)} records.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Let's create features that will help our models capture the time series patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series features\n",
    "target_column = 'vehicle_usage_hours'\n",
    "lag_features = [1, 2, 3, 7, 14, 30]  # Previous day, week, and month\n",
    "rolling_features = [7, 14, 30]  # Weekly, bi-weekly, and monthly windows\n",
    "rolling_stats = ['mean', 'std', 'min', 'max']\n",
    "\n",
    "df_features = create_features(\n",
    "    df, \n",
    "    target_column=target_column,\n",
    "    lag_features=lag_features,\n",
    "    rolling_features=rolling_features,\n",
    "    rolling_stats=rolling_stats\n",
    ")\n",
    "\n",
    "print(f\"Created features dataframe with {df_features.shape[1]} columns.\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values (due to lag features)\n",
    "df_features = df_features.dropna()\n",
    "print(f\"After dropping NaN values, dataframe has {len(df_features)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoded features for categorical variables\n",
    "df_features = pd.get_dummies(df_features, columns=['vehicle_type', 'region', 'season'], drop_first=False)\n",
    "\n",
    "# List the engineered features\n",
    "feature_columns = df_features.columns.tolist()\n",
    "feature_columns.remove(target_column)  # Remove the target column\n",
    "\n",
    "print(f\"Total number of features: {len(feature_columns)}\")\n",
    "print(\"\\nSample of engineered features:\")\n",
    "print(feature_columns[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split\n",
    "\n",
    "Let's split our data into training and testing sets, respecting the time series nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "test_size = 0.2  # Use 20% of the data for testing\n",
    "(X_train, y_train), (X_test, y_test) = train_test_split_time(\n",
    "    df_features, test_size=test_size, target_column=target_column\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train-test split\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_train.index, y_train, label='Training Data')\n",
    "plt.plot(y_test.index, y_test, label='Testing Data', color='red')\n",
    "plt.title('Train-Test Split for Vehicle Usage Hours')\n",
    "plt.ylabel('Hours')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Models\n",
    "\n",
    "Let's implement and evaluate several time series forecasting models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Baseline Models\n",
    "\n",
    "We'll start with simple baseline models to establish a performance benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive forecast (use the last value)\n",
    "naive_forecast = y_train.iloc[-1]\n",
    "y_pred_naive = pd.Series(naive_forecast, index=y_test.index)\n",
    "\n",
    "# Seasonal naive forecast (use the value from the same day of the previous week)\n",
    "y_pred_seasonal_naive = pd.Series(index=y_test.index)\n",
    "for i, idx in enumerate(y_test.index):\n",
    "    # Find the same day of the week from the previous week\n",
    "    prev_week = idx - pd.Timedelta(days=7)\n",
    "    if prev_week in y_train.index:\n",
    "        y_pred_seasonal_naive[idx] = y_train[prev_week]\n",
    "    else:\n",
    "        # If not available, use the last value from training\n",
    "        y_pred_seasonal_naive[idx] = y_train.iloc[-1]\n",
    "\n",
    "# Average forecast (use the average of the training data)\n",
    "average_forecast = y_train.mean()\n",
    "y_pred_average = pd.Series(average_forecast, index=y_test.index)\n",
    "\n",
    "# Evaluate baseline models\n",
    "baseline_results = {\n",
    "    'Naive': evaluate_forecast(y_test, y_pred_naive),\n",
    "    'Seasonal Naive': evaluate_forecast(y_test, y_pred_seasonal_naive),\n",
    "    'Average': evaluate_forecast(y_test, y_pred_average)\n",
    "}\n",
    "\n",
    "# Display results\n",
    "baseline_df = pd.DataFrame(baseline_results).T\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline forecasts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_train.index[-30:], y_train[-30:], label='Training Data', color='blue')\n",
    "plt.plot(y_test.index, y_test, label='Actual', color='black')\n",
    "plt.plot(y_test.index, y_pred_naive, label='Naive', color='red', linestyle='--')\n",
    "plt.plot(y_test.index, y_pred_seasonal_naive, label='Seasonal Naive', color='green', linestyle='--')\n",
    "plt.plot(y_test.index, y_pred_average, label='Average', color='purple', linestyle='--')\n",
    "plt.title('Baseline Forecasts vs Actual Values')\n",
    "plt.ylabel('Vehicle Usage Hours')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ARIMA Models\n",
    "\n",
    "Now let's implement ARIMA (AutoRegressive Integrated Moving Average) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best ARIMA model using auto_arima\n",
    "auto_arima_model = auto_arima(\n",
    "    y_train,\n",
    "    seasonal=True,\n",
    "    m=7,  # Weekly seasonality\n",
    "    information_criterion='aic',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the order and seasonal_order from auto_arima\n",
    "arima_order = auto_arima_model.order\n",
    "seasonal_order = auto_arima_model.seasonal_order\n",
    "\n",
    "print(f\"Best ARIMA order: {arima_order}\")\n",
    "print(f\"Best seasonal order: {seasonal_order}\")\n",
    "\n",
    "# Fit SARIMA model with the best parameters\n",
    "sarima_model = SARIMAX(\n",
    "    y_train,\n",
    "    order=arima_order,\n",
    "    seasonal_order=seasonal_order,\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "sarima_fit = sarima_model.fit(disp=False)\n",
    "print(sarima_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast with SARIMA model\n",
    "sarima_forecast = sarima_fit.forecast(steps=len(y_test))\n",
    "y_pred_sarima = pd.Series(sarima_forecast, index=y_test.index)\n",
    "\n",
    "# Evaluate SARIMA model\n",
    "sarima_results = evaluate_forecast(y_test, y_pred_sarima)\n",
    "print(\"SARIMA Model Results:\")\n",
    "for metric, value in sarima_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SARIMA forecast\n",
    "plot_forecast(y_test, y_pred_sarima, train_actual=y_train[-30:], \n",
    "              title='SARIMA Forecast vs Actual Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Machine Learning Models\n",
    "\n",
    "Let's implement machine learning models that can leverage our engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Regressor\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = pd.Series(xgb_model.predict(X_test), index=y_test.index)\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "xgb_results = evaluate_forecast(y_test, y_pred_xgb)\n",
    "print(\"XGBoost Model Results:\")\n",
    "for metric, value in xgb_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot XGBoost forecast\n",
    "plot_forecast(y_test, y_pred_xgb, train_actual=y_train[-30:], \n",
    "              title='XGBoost Forecast vs Actual Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Selection\n",
    "\n",
    "Let's compare all the models and select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {\n",
    "    'Naive': baseline_results['Naive'],\n",
    "    'Seasonal Naive': baseline_results['Seasonal Naive'],\n",
    "    'Average': baseline_results['Average'],\n",
    "    'SARIMA': sarima_results,\n",
    "    'XGBoost': xgb_results\n",
    "}\n",
    "\n",
    "# Create a comparison dataframe\n",
    "comparison_df = pd.DataFrame(all_results).T\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=comparison_df.index, y='RMSE', data=comparison_df)\n",
    "plt.title('RMSE Comparison Across Models')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all forecasts together\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(y_train.index[-30:], y_train[-30:], label='Training Data', color='blue', alpha=0.5)\n",
    "plt.plot(y_test.index, y_test, label='Actual', color='black', linewidth=2)\n",
    "plt.plot(y_test.index, y_pred_naive, label='Naive', linestyle='--')\n",
    "plt.plot(y_test.index, y_pred_seasonal_naive, label='Seasonal Naive', linestyle='--')\n",
    "plt.plot(y_test.index, y_pred_sarima, label='SARIMA', linestyle='--')\n",
    "plt.plot(y_test.index, y_pred_xgb, label='XGBoost', linestyle='--')\n",
    "plt.title('All Forecasts vs Actual Values')\n",
    "plt.ylabel('Vehicle Usage Hours')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation\n",
    "\n",
    "Let's interpret the best model and understand what drives our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model based on RMSE\n",
    "best_model = comparison_df['RMSE'].idxmin()\n",
    "print(f\"The best model based on RMSE is: {best_model}\")\n",
    "\n",
    "# If XGBoost is the best model, analyze feature importance in more detail\n",
    "if best_model == 'XGBoost':\n",
    "    # Get top 10 features\n",
    "    top_features = feature_importance.head(10)['Feature'].tolist()\n",
    "    \n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    for i, (feature, importance) in enumerate(zip(feature_importance.head(10)['Feature'], \n",
    "                                                 feature_importance.head(10)['Importance'])):\n",
    "        print(f\"{i+1}. {feature}: {importance:.4f}\")\n",
    "    \n",
    "    # Analyze how these features relate to the target\n",
    "    print(\"\\nCorrelation with target:\")\n",
    "    for feature in top_features:\n",
    "        if feature in df_features.columns:\n",
    "            corr = df_features[feature].corr(df_features[target_column])\n",
    "            print(f\"{feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If SARIMA is the best model, analyze the components\n",
    "if best_model == 'SARIMA':\n",
    "    # Plot the components\n",
    "    sarima_fit.plot_components(figsize=(12, 10))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Best Model\n",
    "\n",
    "Let's save the best model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = os.path.join('..', 'models')\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Save the best model\n",
    "if best_model == 'XGBoost':\n",
    "    import joblib\n",
    "    # Save the model\n",
    "    joblib.dump(xgb_model, os.path.join(models_dir, 'xgboost_model.pkl'))\n",
    "    # Save feature names\n",
    "    with open(os.path.join(models_dir, 'feature_names.json'), 'w') as f:\n",
    "        json.dump(X_train.columns.tolist(), f)\n",
    "    print(f\"Saved XGBoost model to {os.path.join(models_dir, 'xgboost_model.pkl')}\")\n",
    "    \n",
    "elif best_model == 'SARIMA':\n",
    "    # Save the model parameters\n",
    "    sarima_params = {\n",
    "        'order': arima_order,\n",
    "        'seasonal_order': seasonal_order,\n",
    "        'params': sarima_fit.params.to_dict()\n",
    "    }\n",
    "    with open(os.path.join(models_dir, 'sarima_model_params.json'), 'w') as f:\n",
    "        json.dump(sarima_params, f, indent=4)\n",
    "    print(f\"Saved SARIMA model parameters to {os.path.join(models_dir, 'sarima_model_params.json')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights from Modeling\n",
    "\n",
    "Based on our modeling efforts, here are the key insights:\n",
    "\n",
    "1. **Model Performance**:\n",
    "   - The [best_model] model performed best with an RMSE of [best_rmse]\n",
    "   - This represents a [improvement_percentage]% improvement over the naive baseline\n",
    "\n",
    "2. **Important Predictors**:\n",
    "   - Recent past values (lag features) are the strongest predictors\n",
    "   - Weekly seasonality is a significant factor\n",
    "   - Vehicle type and region also influence usage patterns\n",
    "\n",
    "3. **Forecasting Accuracy**:\n",
    "   - The model can predict vehicle usage with a mean absolute percentage error of approximately [mape]%\n",
    "   - Predictions are more accurate for short-term forecasts (1-7 days) than long-term forecasts\n",
    "\n",
    "4. **Business Implications**:\n",
    "   - The model can help optimize fleet allocation based on predicted demand\n",
    "   - Understanding the key drivers of vehicle usage can inform strategic decisions\n",
    "   - The forecasting capability enables better planning for maintenance and resource allocation\n",
    "\n",
    "In the next notebook, we'll use these models for forecasting and develop a comprehensive report with actionable recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
